---
title: "NYT Article and Comment Text Mining & Political News Prediction"
author: "Julia Ulziisaikhan"
date: "5/7/2022"
output: html_document
---

# Table of Contents

0. Required Packages and Data

1. Raw Data Exploratory Analysis

    1.a. Comments: Sentiment Analysis and Word Clouds

    1.b. Articles: NYT's Categorization

    1.c. Missing Data

2. Data Processing

    2.a. Comment Section Sentiment Makeup

    2.b. Polarization Metric

    2c. Political Content Lexicon

3. Processed Data Analysis

    3.a. General Exploration

    3.b. Evaluation of Polarization Metric

    3.c. Subsetting (and out) Op-Ed Articles

4. Binary Prediction: Is the article political or not?

    4.a. Logistic Regression

    4.b. Lasso Regression

    4.c. Naive Bayes Regression

    4.d. Comparison of Models' Performance
    
    4.e. Subsetting (and out) Op-Ed Articles for Prediction

# 0. Required Packages and Data

```{r, warning=F, message=F}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(janitor)
library(caret)
library(glmnet)
```


# 1. Raw Data Exploratory Analysis

```{r}
month <- "April2017.csv"
df_a_eda <- read.csv(paste0("Articles", month)) %>% clean_names()
df_c_eda <- read.csv(paste0("Comments", month)) %>% clean_names()
```

```{r}
head(df_a_eda)
```


```{r}
colnames(df_a_eda)
```

Data summary function:

```{r}
nas <- c("", " ", "n/a", "NA", "N/A", "unknown", "Unknown", NA)
print("colname | prop_nas | top 3 vals")
print("------------------")
for(i in 1:length(colnames(df_a_eda))){
  colname <- colnames(df_a_eda)[i]
  num_unique <- length(unique(df_a_eda$colname))
  num_nas <- pull(df_a_eda, i) %in% nas %>% sum()
  prop_nas <- num_nas / nrow(df_a_eda) 
  if (num_unique > 2 | num_unique < 51){
    top3 <- 
      df_a_eda %>% group_by({{colname}}) %>% summarise(n=n()) %>% 
      arrange(desc(n)) %>% pull(1) %>% head(3) %>% paste(collapse = ", ")
  } else if (num_unique < 3) {
    top3 <- "too few cats"
  } else if (num_unique > 50) {
    top3 <- "too many cats"
  }

  str <- paste(colname, "| ", prop_nas %>% round(2), "|", top3)
  print(str)
}
```


```{r}
colnames(df_c_eda)
```


# 2. Data Processing

Loading in all files.

```{r}
path <- "C:/Users/ulzii/OneDrive/Documents/spring22/data_mining/0_final_project"
if (getwd() == path) {
  art_li <- lapply(Sys.glob("Articles*.csv"), read.csv) 
  com_li <- lapply(Sys.glob("Comments*.csv"), read.csv) 
} 
```

We include only the variables we need to save memory.

```{r}
cols <- 
  c("article_id, keywords, pub_date, article_word_count, headline, type_of_material", "snippet", "web_url") %>%
  strsplit(., ", ") %>%
  unlist()

cols2 <- 
  c("article_id, user_id, comment_body, approve_date") %>%
  strsplit(., ", ") %>%
  unlist()

for (i in 1:9){
  art_li[[i]] <- subset(art_li[[i]] %>% janitor::clean_names(), select=cols)
  com_li[[i]] <- subset(com_li[[i]] %>% janitor::clean_names(), select=cols2)
}

# Removing all objects in environment except for our data
rm(list=setdiff(ls(), c("art_li", "com_li")))
```

## 2.a. Comment Section Sentiment Makeup

Here we create our new variables.

We turn our text data that was in string format to single-token-per-row format, using `unnest_tokens()`. We also remove stop words.

```{r}
#Function to view word-level sentiment
tidytext_fmt <- 
  function(data, lexicon="afinn"){
    data %>%
    janitor::clean_names() %>% 
    unnest_tokens(tbl=., 
                  output=word, 
                  input=comment_body, 
                  to_lower = TRUE) %>%
    dplyr::anti_join(stop_words, by="word")  %>% 
    inner_join(get_sentiments(lexicon), by="word")
  }

#Function to view comment-level sentiment
sentiment_anl <- function(data){
  tidytext_fmt(data) %>%
  group_by(article_id, user_id) %>%
  summarize(value = sum(value)) %>%
  suppressMessages() %>%
  mutate(sentiment = case_when(
  value > 0 ~ "pos",
  value == 0 ~ "neu",
  value < 0 ~ "neg"))
}
```

## 2.b. Polarization Metric

The `polarization` metric is calculated by taking `neg` (the proportion of comments of negative sentiment per article), dividing it by the `pos`, positive sentiment proportion of comments, then subtracting 1. Then you add `neu`, neutral sentiment proportion of comments, to that number. This means that the closer `polarization` is to 0, the more polarization it has.

```{r}
#Function to view article-level sentiment
article_bkdwn <- function(data, pivot=TRUE){
  
  df <- 
    sentiment_anl(data) %>%
    group_by(article_id, sentiment) %>% 
    summarize(n=n()) %>%
    suppressMessages() %>%
    group_by(article_id) %>%
    dplyr::mutate(num_comments=sum(n), percent = n / num_comments)
  
  if (pivot==TRUE){
    df %>% 
    dplyr::select(-n) %>%
    tidyr::pivot_wider(data=., 
                names_from = sentiment, 
                values_from = percent) %>%
    # CALCULATION OF POLARIZATION SCORE:
    mutate(polarization = (neg / pos - 1) + neu )
    }
  
  else if (pivot==FALSE){
    df
  }
}

#String cleaner
string_cleaner <-
  function(string){
    #removes punc
    str_replace_all(string, "[[:punct:]]", "") %>%
      #removes slashes
      gsub("[^0-9A-Za-z///' ]","'" , ., ignore.case = FALSE) %>%
      gsub('"', "", .) %>%
      gsub("'", "", .) %>%
      #turns to lower case
      tolower()
  }

# Date Time function
date_time <- 
  function(string){
    as.POSIXct(string, format="%Y-%m-%d %H:%M:%S", tz="UTC")
  }

#Main function
processor <- 
  function(comment_data, article_data){
  comment_data <- article_bkdwn(comment_data)
  article_data <- article_data %>% janitor::clean_names()
  left_join(article_data, comment_data, by="article_id") %>%
    filter(!is.na(polarization) & !is.na(neu)) %>% 
    mutate(
      pub_date = date_time(pub_date),
      headline = string_cleaner(headline),
      keywords = string_cleaner(keywords),
      snippet = string_cleaner(snippet),
      oped = ifelse(type_of_material=="Op-Ed", 1, 0),
      id = article_id,
      word_count = article_word_count) %>%
    select(-article_id, -article_word_count, -type_of_material) %>% 
    relocate(id, .before = keywords) %>%
    relocate(keywords, .after=word_count) %>%
    relocate(snippet, .after=keywords) %>%
    relocate(headline, .after=snippet) %>%
    relocate(pub_date, .before=keywords) %>% 
    relocate(word_count, .before=num_comments) %>%
    relocate(web_url, .after=pub_date) 
  }
```


```{r}
for (i in 1:9){
  vars <- paste("df", i, sep="")
  assign(vars, 
         processor(com_li[[i]], art_li[[i]])
         )
  message("processed df", i, " of 9")
}
```

We create a processed data file, `processed_data.csv`, and remove all variables in environment to conserve space.

```{r}
df <- 
  rbind(df1,df2,df3,df4,df5,df6,df7,df8,df9) %>% 
  write.csv(., file="processed_data.csv")
```

```{r}
rm(list = ls())
```

## 2c. Political Content Lexicon

```{r}
read.csv("political_lexicon.csv") %>% 
    pull(1) %>% head()
```

```{r}
# Political Classifier Function
pol_classifier <- function(string){
  pol_lex <- 
    read.csv("political_lexicon.csv") %>% 
    pull(1) %>% 
    paste(collapse="|")
  bool <- grepl(pol_lex, string)
  ifelse(bool == TRUE, 1, 0)
}
```

```{r}
df <- 
  read.csv("processed_data.csv") %>% 
  mutate(political = pol_classifier(keywords)) %>%
  select(-X) %>%
  relocate(political, .after=oped) %>%
  unique # somehow duplicate rows are entered 

# write.csv(df, file="processed_data.csv")

glimpse(df)
```

# 3. Processed Data Analysis

## 3.a. General Exploration

```{r}
paste("# of articles in data:", nrow(df))
```

```{r}
pct <- function(decimal, places = 2){
  output <- round((decimal * 100), places)
  return(paste0(output, "%"))
}
```

Of all articles...

```{r}
df %>% group_by(oped) %>% summarise(n = n()) %>% mutate(pct = pct(n/nrow(df)))
```

```{r}
df %>% group_by(political) %>% summarise(n = n()) %>% mutate(pct = pct(n/nrow(df)))
```

Of political articles...

```{r}
filter(df, political == 1) %>% group_by(oped) %>% summarise(n = n()) %>% mutate(pct = pct(n/nrow(df)))
```

Of non-political articles...

```{r}
filter(df, political == 0) %>% group_by(oped) %>% summarise(n = n()) %>% mutate(pct = pct(n/nrow(df)))
```

## 3.b. Evaluation of Polarization Metric

## 3.c. Subsetting (and out) Op-Ed Articles


# 4. Binary Prediction: Is the article political or not?

Removing features not needed for modelling:

Models cannot process POSIX objects, so we take out pub_date. 
Also remove any text body variables, such as id, keywords, snippet, and headline.
We remove the prediction variable of interest, as we will contain it in vector y elsewhere.

```{r}
df_m <- 
  df %>%
  select(-pub_date, -id, -keywords, -snippet, -headline, -web_url, -political) 

glimpse(df_m)
```

Test-train split

```{r}
class_1 <- which(y == 1)
class_0 <- which(y == 0)

political_prop <- 0.59
class_1_test <- sample(class_1, size = 59, replace = FALSE)
class_0_test <- sample(class_0, size = 41, replace = FALSE)

test_indices <- c(class_1_test, class_0_test)
test_split <- (1:nrow(df_m) %in% test_indices)
train_split <- !test_split
```

Metrics function, to assess quality of predictions

```{r}
extract_metrics <- function(conf_matrix) {
    metrics <- c(
        1 - conf_matrix$overall['Accuracy'],
        conf_matrix$byClass[c('Precision', 'Recall')]
    )
    names(metrics) <- c(
        'Classification_Error', 'Precision', 'Recall'
    )
    return(metrics)
}
```

Score to prediction function, to convert any values which are greater than 0.5 to give a predicted class of 1, and any values which are less than or equal to 0.5 to give a predicted class of 0

```{r}
score_to_pred <- function(score) {
    return(ifelse(score > 0.5, 1, 0))
}
```

Random classification.

```{r}
y_pred <- rbinom(100, 1, political_prop)

random_class <- confusionMatrix(
    data = as.factor(y_pred), reference = as.factor(y[test_split]),
    mode = "prec_recall", positive = '1'
)

(random_metrics <- extract_metrics(random_class))
```


## 4.a. Logistic Regression

```{r}
logistic <- glm(y ~ .^2, data=cbind(df_m[train_split, ],  y = y[train_split]), family = binomial)

fit = glm(vs ~ hp, data=mtcars, family=binomial)
newdat <- data.frame(hp=seq(min(mtcars$hp), max(mtcars$hp),len=100))
newdat$vs = predict(fit, newdata=newdat, type="response")
plot(vs~hp, data=mtcars, col="red4")
lines(vs ~ hp, newdat, col="green4", lwd=2)


y_pred <- predict(logistic, newdata = data.frame(as.matrix(df_m[test_split, ])))


logistic_class <- confusionMatrix(
    data = as.factor(y_pred), reference = as.factor(y[test_split]),
    mode = "prec_recall", positive = '1'
)

(logistic_metrics <- extract_metrics(logistic_class))
```


## 4.b. Lasso Regression

```{r}
ls <- cv.glmnet(
    as.matrix(df_m[train_split, ]), y[train_split],
    alpha = 1, lambda = 10^seq(-5, 0, length.out=50)
)
plot(ls)
```

```{r}
k <- which(ls$lambda == ls$lambda.1se)
ls_coefs <- ls$glmnet.fit$beta[, k]
print(paste(
    'Proportion of 0 coefficients:', mean(ls_coefs == 0)
))
```

```{r}
y_pred <- score_to_pred(
    predict(ls, as.matrix(df_m[test_split, ]))
)


lasso_class <- confusionMatrix(
    data = as.factor(y_pred), reference = as.factor(y[test_split]),
    mode = "prec_recall", positive = '1'
)

(lasso_metrics <- extract_metrics(lasso_class))
```



## 4.c. Naive Bayes Regression

## 4.d. Comparison of Models' Performance

```{r}
random_metrics
```

```{r}
lasso_metrics
```

```{r}
logistic_metrics
```


## 4.e. Subsetting (and out) Op-Ed Articles for Prediction